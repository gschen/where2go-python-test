import tensorflow as tf
import numpy1 as np

#Broadcasting
#优化手段，应用于不同行列数之间的矩阵相加
'''
    张量维度的扩张，对某一个维度重复n多次，但却没有真正的重复其中的数值
    与tf.tile区别，tf.tile将某个维度重复多次并且会重复值
    举例
    [b,784]*[784,10]+[10]=[b,10]+[10]  
    不可直接相加，此时就需要将[10]变为[b,10]
    =[b,10]+[b,10]
    但又不能因为数据扩张影响结果，所以就有Broadcasting
    用Broadcasting将[10]变为[b,10],此时的b维度并没有真正的值
'''
'''
    a[4,16,16,32]
    b        [32]
    右对齐
    将b变为[1,1,1,32]
    在变换对应的维度b=[4,16,16,32]
'''
'''
    实际运算举例
    [ 0, 0, 0]      [ 0, 1, 2]      [ 0, 0, 0]      [ 0, 1, 2]   [ 0, 1, 2]
    [10,10,10]  +                =  [10,10,10]  +   [ 0, 1, 2] = [10,11,12]
    [20,20,20]                      [20,20,20]      [ 0, 1, 2]   [20,21,22]
    [30,30,30]                      [30,30,30]      [ 0, 1, 2]   [30,31,32]
                                                    此时多出来的三行并未占用内存空间，不是真的复制数值，但不影响计算
    
    [ 0, 0, 0]      [5]         [ 0, 0, 5]
    [10,10,10]  +           =   [10,10,15]
    [20,20,20]                  [20,20,25]
    [30,30,30]                  [30,30,35]     
    
    假设这次考试英语试卷的某道题改错了，要给所有人加5分
    运用Broadcasting可以达到如下效果
    [classes,students,scores]  + [scores] [scores]的内容   [0]
                                                          [0]
                                                          [5]
                                                          [0]
                                                          ...                                
'''
#好处：不是对数组的真实扩张，是内部的一种优化计算，所以不占用内存
'''
判断是否能Broadcasting
右对齐，且对应位置的维度shape要相等
如[1,3]和[4],4和3不等，不能Broadcasting
如[4,32,14,14]和[2,32,14,14],第一位不想等，且需要变换的第一个维度的shape不为1,所以不行
如[4,32,14,14]和[1,32,14,14]就可以
'''

a=tf.random.normal([4,32,32,3])
(a+tf.random.normal([3]))#遵循Broadcasting
#虽然没用到tf.broadcasting_to,但只要该数据支持Broadcasting系统就会自动调用broadcasting
(a+tf.random.normal([4,1,1,1]))#成立且遵循Broadcasting
(a+tf.random.normal([1,4,1,1]))#不成立
#采用tf.broadcast_to变换
b=tf.broadcast_to(tf.random.normal([4,1,1,1]),[4,32,32,3])
#tf.broadcast_to(需要变换的对象，变换为什么形状)

#方法1
a=tf.ones([3,4])
a1=tf.broadcast_to(a,[2,3,4])#返回的shape为[2,3,4]
#方法2
a2=tf.expand_dims(a,axis=0)#[1,3,4]
a2=tf.tile(a2,[2,1,1])#[2,3,4],用[2,1,1]去乘对应位的[1,3,4]，得到[2,3,4]

#方法1和2，用于计算能得到同样的结果，但方法1不会占用多的内存空间，且更简单

